__all__ = ()


from collections.abc import Sequence
from datetime import datetime, timedelta, timezone
from itertools import product
import json
import logging
import math
from pathlib import Path
import time
from typing import Any, Iterable, Literal

import numpy as np
import numpy.typing as npt
from pydantic import PositiveInt
from scipy import linalg, stats
from scipy.stats.qmc import Halton

from ..constants import _SAMPLES_SIMULATIONS_RATIO
from ..logging import get_script_logger
from ._helpers import _format_resource_bounds
from ._submit import _submit_scenario_job
from .systems import BatchSystem
from .types import JobResources, JobResult, JobSize


def _estimate_upper_bound(
    X: npt.NDArray[np.float64],
    y: npt.NDArray[np.float64],
    x: npt.NDArray[np.float64],
    pred_interval: float,
) -> float:
    """
    Estimate the upper bound of a linear regression.

    This helper function calculates the upper bound for resources required for a job
    given some description of the job size. The upper bound is calculated using a
    multivariate linear regression with a prediction interval and `pred_interval`
    controls the interval.

    Args:
        X: The independent variables.
        y: The dependent variables.
        x: The independent variables to estimate the upper bound for.
        pred_interval: The prediction interval to use.

    Returns:
        The estimated upper bound of the linear regression.
    """
    n, k = X.shape
    beta, _, _, _ = linalg.lstsq(X, y)
    mse = np.sum((y - np.dot(X, beta)) ** 2) / (n - k - 1)
    se = np.sqrt(mse * (1 + np.dot(x, np.dot(linalg.inv(np.dot(X.T, X)), x))))
    t = stats.t.ppf(0.5 * (pred_interval + 1), n - k - 1)
    return np.dot(x, beta) + (t * se)


def _create_estimate_job_size_from_reference(
    reference_job_size: JobSize,
    overrides: dict[
        Literal["blocks", "chains", "samples", "simulations"], PositiveInt | None
    ],
) -> JobSize:
    """
    Create an estimate job size from a reference job size.

    Args:
        reference_job_size: The reference job size to use.
        overrides: The overrides to apply to the reference job size.

    Returns:
        The estimated job size.
    """
    kwargs = (
        reference_job_size.model_dump(
            include=("blocks", "chains", "samples", "simulations")
        )
        | overrides
    )
    if (
        (samples := kwargs.get("samples")) is not None
        and (simulations := kwargs.get("simulations")) is not None
        and samples > simulations
    ):
        kwargs["samples"] = max(math.floor(_SAMPLES_SIMULATIONS_RATIO * simulations), 1)
    return JobSize(**kwargs)


def _generate_job_sizes_grid(
    reference_job_size: JobSize,
    vary_fields: Sequence[Literal["blocks", "chains", "simulations"]],
    lower_scale: float | int,
    upper_scale: float | int,
    estimate_runs: PositiveInt,
    verbosity: int,
) -> list[JobSize]:
    """
    Generate a grid of job sizes for resource estimation.

    This function generates a grid of job sizes that are between a 10th and a 3rd of the
    original job size. The grid is generated using a Halton sequence and the job sizes
    are generated by taking the maximum of the original job size divided by 10 and 3.

    Args:
        reference_job_size: The reference job size to generate the grid from.
        estimate_runs: The number of runs to generate.
        verbosity: The verbosity level of the estimation.

    Returns:
        A list of job sizes for resource estimation.

    Raises:
        ValueError: If any of the 'blocks', 'chains', or 'simulations' fields are `None`
            in the reference job size.
    """
    logger = get_script_logger(__name__, verbosity)
    if not vary_fields:
        raise ValueError(f"The vary fields must not be empty.")
    if lower_scale <= upper_scale or math.isclose(lower_scale, upper_scale):
        raise ValueError(
            f"The lower scale, {lower_scale}, must be "
            f"greater than the upper scale, {upper_scale}."
        )
    if none_fields := [f for f in vary_fields if getattr(reference_job_size, f) is None]:
        none_fields = "'" + "', '".join(none_fields) + "'"
        raise ValueError(
            "The reference job size has `None` for the following fields "
            f"which is not allowed for estimation: {none_fields}."
        )
    l_bounds = [
        max(math.floor(getattr(reference_job_size, f) / lower_scale), 1)
        for f in vary_fields
    ]
    u_bounds = [
        max(math.ceil(getattr(reference_job_size, f) / upper_scale), 1) for f in vary_fields
    ]
    logger.info(
        "Generating %u job sizes between %s and %s in size.",
        estimate_runs,
        _create_estimate_job_size_from_reference(
            reference_job_size, dict(zip(vary_fields, l_bounds))
        ),
        _create_estimate_job_size_from_reference(
            reference_job_size, dict(zip(vary_fields, u_bounds))
        ),
    )
    engine = Halton(len(vary_fields))
    samples = engine.integers(
        l_bounds=l_bounds, u_bounds=u_bounds, n=estimate_runs, endpoint=True
    )
    estimate_job_sizes = [
        _create_estimate_job_size_from_reference(
            reference_job_size, dict(zip(vary_fields, x))
        )
        for x in samples
    ]
    logger.info(
        "Estimating resources for %u job sizes between a 10th and a 3rd of "
        "the original job size.",
        estimate_runs,
    )
    for i, estimate_job_size in enumerate(estimate_job_sizes):
        logger.debug("Estimation job %u has size %s.", i, estimate_job_size)
    return estimate_job_sizes


def _submit_and_poll_estimate_jobs(
    estimate_job_sizes: list[JobSize],
    name: str,
    job_name: str,
    inference_method: Literal["emcee", "r"],
    time_out_limit: timedelta,
    batch_system: BatchSystem,
    outcome_modifiers_scenarios: list[str],
    seir_modifiers_scenarios: list[str],
    options: dict[str, str | Iterable[str]] | None,
    general_template_data: dict[str, Any],
    estimate_runs: int,
    estimate_interval: float,
    verbosity: int,
    dry_run: bool,
) -> dict[int, JobResult] | None:
    """
    Submit and poll for estimation jobs.

    Args:
        estimate_job_sizes: The job sizes to estimate resources for.
        name: The name of the config file used as a prefix for the job name.
        job_name: The name of the job to submit.
        inference_method: The inference method being used.
        time_out_limit: The time limit to wait for the estimation jobs to finish.
        batch_system: The batch system to submit the job to.
        outcome_modifiers_scenarios: The outcome modifiers scenarios to use.
        seir_modifiers_scenarios: The SEIR modifiers scenarios to use.
        options: Additional options to pass to the batch system.
        general_template_data: The general template data to use for the job submission.
        estimate_runs: The number of runs to use for the estimation.
        estimate_interval: The prediction interval to use for the estimation.
        verbosity: The verbosity level of the submission.
        dry_run: Whether to perform a dry run of the submission.

    Returns:
        The results of the estimation jobs as a dict keyed on the hash of the job size,
        outcome modifiers scenario, and SEIR modifiers scenario or `None` if a dry run.
    """
    logger = get_script_logger(__name__, verbosity)

    submissions = {}
    for i, estimate_job_size in enumerate(estimate_job_sizes):
        general_template_data["job_size"] = estimate_job_size.model_dump()
        for outcome_modifiers_scenario, seir_modifiers_scenario in product(
            outcome_modifiers_scenarios, seir_modifiers_scenarios
        ):
            submission = _submit_scenario_job(
                name,
                f"{job_name}_estimate_{i}",
                inference_method,
                estimate_job_size,
                batch_system,
                outcome_modifiers_scenario,
                seir_modifiers_scenario,
                options,
                general_template_data,
                verbosity,
                dry_run,
            )
            submissions[
                hash(
                    (
                        frozenset(estimate_job_size),
                        outcome_modifiers_scenario,
                        seir_modifiers_scenario,
                    )
                )
            ] = submission
            if not dry_run:
                logger.debug(
                    "Submitted job %i with size %s for outcome modifier "
                    "scenario '%s' and SEIR modifier scenario '%s'.",
                    submission.job_id,
                    estimate_job_size,
                    outcome_modifiers_scenario,
                    seir_modifiers_scenario,
                )

    if dry_run:
        logger.info(
            "If not dry run, would have waited for %u estimates to "
            "finish then calculated the resource estimation using "
            "%.2f%% prediction interval.",
            estimate_runs,
            100.0 * estimate_interval,
        )
        return None
    else:
        logger.info(
            "Waiting for %u estimates to finish, then will calculate the "
            "resource estimation using %.2f%% prediction interval.",
            estimate_runs,
            100.0 * estimate_interval,
        )

    results = {}
    start = datetime.now(timezone.utc)
    logger.info(
        "Starting to poll for submission jobs, will timeout at %s.",
        (start + time_out_limit).strftime("%c %Z"),
    )
    while len(results) < len(submissions):
        time.sleep(120.0)
        for key, submission in submissions.items():
            if key in results:
                continue
            result = batch_system.status(submission)
            if result is not None and result.status in {"completed", "failed"}:
                logger.log(
                    logging.INFO if result.status == "completed" else logging.WARNING,
                    "Test job %i finished with status '%s'.",
                    submission.job_id,
                    result.status,
                )
                results[key] = result
        if datetime.now(timezone.utc) - start > time_out_limit:
            seconds_limit = math.ceil(time_out_limit.total_seconds())
            raise TimeoutError(
                "Timed out waiting for estimation jobs "
                f"to finish after {seconds_limit} seconds."
            )
    logger.info("All estimation jobs have finished.")

    return results


def _collect_submission_results(
    estimate_factors: Iterable[str],
    estimate_measurements: Iterable[Literal["cpu", "memory", "time"]],
    estimate_interval: float,
    reference_job_size: JobSize,
    reference_job_resources: JobResources,
    estimate_job_sizes: list[JobSize],
    outcome_modifiers_scenarios: list[str],
    seir_modifiers_scenarios: list[str],
    submission_results: dict[int, JobResult],
    resources_file: Path | None,
    verbosity: int,
) -> None:
    """
    Collect submission results and estimate upper bounds for resources.

    Args:
        estimate_factors: The job size fields to use for estimation.
        estimate_measurements: The job size measurements to estimate.
        estimate_interval: The prediction interval to use for the estimation.
        reference_job_size: The reference job size to use for estimation.
        reference_job_resources: The reference job resources to use for estimation.
        estimate_job_sizes: The job sizes to estimate resources for.
        outcome_modifiers_scenarios: The outcome modifiers scenarios to use.
        seir_modifiers_scenarios: The SEIR modifiers scenarios to use.
        submission_results: The results of the estimation jobs.
        resources_file: The file to write the estimated resources to or `None` to not
            write the estimated resources.
        verbosity: The verbosity level of the submission.

    Returns:
        None

    Raises:
        ValueError: If the estimate factors are empty.
        ValueError: If the estimate factors are not valid job size fields.
        ValueError: If the estimate measurements are empty.
    """
    logger = get_script_logger(__name__, verbosity)

    if not estimate_factors:
        raise ValueError("The estimate factors must not be empty.")
    valid_estimate_factors = (
        JobSize.model_fields.keys() | JobSize.model_computed_fields.keys()
    )
    if invalid_estimate_factors := set(estimate_factors) - valid_estimate_factors:
        invalid_estimate_factors = "'" + "', '".join(invalid_estimate_factors) + "'"
        raise ValueError(
            f"The estimate factors {invalid_estimate_factors} "
            "are not valid job size fields."
        )
    if not estimate_measurements:
        raise ValueError("The estimate measurements must not be empty.")

    y_bounds = dict(zip(estimate_measurements, len(estimate_measurements) * [0.0]))

    for outcome_modifiers_scenario, seir_modifiers_scenario in product(
        outcome_modifiers_scenarios, seir_modifiers_scenarios
    ):
        x = []
        y = []

        for i in range(len(estimate_job_sizes)):
            key = hash(
                (
                    frozenset(estimate_job_sizes[i]),
                    outcome_modifiers_scenario,
                    seir_modifiers_scenario,
                )
            )
            result = submission_results[key]
            if result.status == "completed":
                model_dump = estimate_job_sizes[i].model_dump(include=estimate_factors)
                x.append([model_dump[ef] for ef in estimate_factors])
                y_i = []
                if "cpu" in estimate_measurements:
                    y_i.append(result.cpu_efficiency * reference_job_resources.cpu)
                if "memory" in estimate_measurements:
                    y_i.append(result.memory_efficiency * reference_job_resources.memory)
                if "time" in estimate_measurements:
                    y_i.append(result.wall_time.total_seconds())
                y.append(y_i)

        logger.debug(
            "Collected %u submissions for outcome modifier scenario '%s' and "
            "SEIR modifier scenario '%s' to estimate resources from.",
            len(x),
            outcome_modifiers_scenario,
            seir_modifiers_scenario,
        )

        X = np.array(x, dtype=np.float64)
        X = np.hstack((np.ones((X.shape[0], 1), dtype=np.float64), X))
        Y = np.array(y, dtype=np.float64)
        x_pred = np.array(
            [1.0] + [reference_job_size.model_dump()[ef] for ef in estimate_factors],
            dtype=np.float64,
        )
        for i, measurement in enumerate(estimate_measurements):
            try:
                y_upper = _estimate_upper_bound(X, Y[:, i], x_pred, estimate_interval)
                y_bounds[measurement] = max(y_bounds[measurement], y_upper)
            except linalg.LinAlgError as e:
                logger.error(
                    "Failed to estimate %s upper bound for outcome modifier scenario "
                    "'%s' and SEIR modifier scenario '%s' using linear regression "
                    "with error: %s",
                    measurement,
                    outcome_modifiers_scenario,
                    seir_modifiers_scenario,
                    e,
                )
        logger.debug(
            "Processed estimation for outcome modifier scenario '%s' "
            "and SEIR modifier scenario '%s' and determined upper bounds "
            "for %s are %s.",
            outcome_modifiers_scenario,
            seir_modifiers_scenario,
            ", ".join(estimate_measurements),
            _format_resource_bounds(y_bounds),
        )

    logger.info(
        "Estimated upper bounds for %s are %s.",
        ", ".join(estimate_measurements),
        _format_resource_bounds(y_bounds),
    )
    if resources_file is not None:
        resources_file.write_text(json.dumps(y_bounds, indent=4))
        logger.info("Wrote estimated resources to '%s'.", resources_file)


def _estimate_job_resources(
    name: str,
    job_name: str,
    inference_method: Literal["emcee", "r"],
    job_size: JobSize,
    job_resources: JobResources,
    time_limit: timedelta,
    batch_system: BatchSystem,
    outcome_modifiers_scenarios: list[str],
    seir_modifiers_scenarios: list[str],
    options: dict[str, str | Iterable[str]] | None,
    general_template_data: dict[str, Any],
    estimate_runs: int,
    estimate_interval: float,
    verbosity: int,
    dry_run: bool,
) -> None:
    """
    Estimate the memory resources and time limit for a production job.

    Loosely this function will:
    1) Construct a set of job sizes that are smaller than the given job size that
       are still representative of the job size (between a 10th and a 3rd of the
       original size).
    2) Submit a job for each of these job sizes and record the time and resources
       used. Can get that info via `batch_system.status`.
    3) Use the time and resources used to estimate the time and resources needed
       for the full job size. Makes this estimate by doing a linear regression and
       taking the upper bound on the prediction interval.

    Args:
        name: The name of the config file used as a prefix for the job name.
        job_name: The name of the job to submit.
        inference_method: The inference method being used.
        job_size: The size of the job to submit.
        job_resources: The resources required for the job to submit.
        time_limit: The time limit of the job to submit.
        batch_system: The batch system to submit the job to.
        outcome_modifiers_scenarios: The outcome modifiers scenarios to use.
        seir_modifiers_scenarios: The SEIR modifiers scenarios to use.
        options: Additional options to pass to the batch system.
        general_template_data: The general template data to use for the job submission.
        estimate_runs: The number of runs to use for the estimation.
        estimate_interval: The prediction interval to use for the estimation.
        verbosity: The verbosity level of the submission.
        dry_run: Whether to perform a dry run of the submission.

    Returns:
        None
    """
    logger = get_script_logger(__name__, verbosity)
    logger.info("Estimating resources for job size of %s", job_size)
    if not batch_system.estimatible:
        raise NotImplementedError(
            f"The batch system '{batch_system.name}' does not support estimation."
        )

    if general_template_data["array_capable"]:
        logger.warning(
            "The inference method '%s' is array capable, but resource "
            "estimation is only designed for non-array jobs. Overriding "
            "array capable to False and nodes to 1.",
            inference_method,
        )
        general_template_data["array_capable"] = False
        general_template_data["nodes"] = 1

    estimate_job_sizes = _generate_job_sizes_grid(
        job_size, ("blocks", "chains", "simulations"), 10, 3, estimate_runs, verbosity
    )

    results = _submit_and_poll_estimate_jobs(
        estimate_job_sizes,
        name,
        job_name,
        inference_method,
        10.0 * time_limit,
        batch_system,
        outcome_modifiers_scenarios,
        seir_modifiers_scenarios,
        options,
        general_template_data,
        estimate_runs,
        estimate_interval,
        verbosity,
        dry_run,
    )
    if results is None:
        return None

    _collect_submission_results(
        ("total_simulations",),
        ("memory", "time"),
        estimate_interval,
        job_size,
        job_resources,
        estimate_job_sizes,
        outcome_modifiers_scenarios,
        seir_modifiers_scenarios,
        results,
        Path.cwd() / f"{name}_resources.json",
        verbosity,
    )

    logger.info("Resource estimation complete.")
